{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39168f5c-77c2-4392-bcdc-6bfbbe6df730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "TEMPERATURE = 0.1  \n",
    "LR = 3e-4         \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MESSAGE = \"test_temperature\" \n",
    "\n",
    "# Transformacije kroz koje prolazi svaka slika dataseta\n",
    "transform = T.Compose([\n",
    "    T.RandomResizedCrop(32),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomApply([T.ColorJitter(0.5, 0.5, 0.5, 0.1)], p=0.8),\n",
    "    T.RandomGrayscale(p=0.2),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "# Klasa koja ce od jedne slike iz dataseta da vrati dve njene augmentacije.\n",
    "# Bitno je primetiti da iako CIFAR10 ima labele mi cemo ih ignorisati \n",
    "# prilikom ucenja modela SimCLR (jer spada u self-supervised learning).\n",
    "class SimCLRDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, _ = self.dataset[index]\n",
    "        return transform(x), transform(x)\n",
    "\n",
    "# Priprema dataseta\n",
    "cifar = torchvision.datasets.CIFAR10(root = \"./data\", train = True, download = True)\n",
    "cifar_small = torch.utils.data.Subset(cifar, np.arange(10000))\n",
    "dataset = SimCLRDataset(cifar_small)\n",
    "dataload = DataLoader(dataset, batch_size = BATCH_SIZE, shuffle = True, drop_last = True, num_workers = 4, pin_memory = True)\n",
    "\n",
    "# Pravimo klasu modela za SimCLR. \n",
    "# Sastoji se od enkodera za koji smo uzeli resnet18 mrezu bez zadnjeg FC sloja (jer ga ne ucimo klasifikaciju).\n",
    "# Na izlaz iz enkodera se nadovezuje projekcioni sloj koji smanjuje dimenziju izlaza enkodera,\n",
    "# ali takodje primenjuje i nelinearnu transformaciju nad podacima.\n",
    "class SimCLRModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        base = resnet18(pretrained = False)\n",
    "\n",
    "        base.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        base.maxpool = nn.Identity()\n",
    "        \n",
    "        self.encoder = nn.Sequential(*list(base.children())[:-1])\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x).squeeze()\n",
    "        z = self.projection(h)\n",
    "        return z\n",
    "\n",
    "# Definisemo model i optimizator. \n",
    "# Bitno da model i podatke s kojima radi cuvamo na istom mestu\n",
    "# zato im svima eksplicitno prosledjujemo DEVICE.\n",
    "model = SimCLRModel().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LR)\n",
    "\n",
    "# Funkcija koja racuna gubitak SimCLR algrotima zasnovana na NT-Xent loss.\n",
    "# zi i zj su izlazi modela za dva susedna batcha. \n",
    "# sim_matrix je matrica slicnosti izmedju slika iz oba batcha. Svako polje Si,j sadrzi kosinusnu slicnost izmedju vektora slika i i j.\n",
    "# Posto je svaki vektor slican sam sebi, na dijagonali ove matrice ce se nalaziti jedinice,\n",
    "# ali to nije podatak koji treba da koristimo za racun zato ga stavljamo na dovoljno mali broj da ne utice na rezultat.\n",
    "# positives je vektor koji za svaki indeks slike prikazuje na kom se indeksu nalazi pozitivni par te slike\n",
    "def calculate_loss(zi, zj, temperature):\n",
    "\n",
    "    n = zi.size(0)\n",
    "    z = torch.cat([zi,zj], dim=0)\n",
    "    z = F.normalize(z, dim=1)\n",
    "\n",
    "    sim_matrix = torch.matmul(z, z.T)\n",
    "    sim_matrix = sim_matrix / temperature\n",
    "\n",
    "    mask = torch.eye(2*n, dtype = torch.bool, device = z.device)\n",
    "    sim_matrix = sim_matrix.masked_fill(mask, -9999999)\n",
    "\n",
    "    positives = torch.cat([torch.arange(n, 2*n), torch.arange(0, n)]).to(DEVICE)\n",
    "\n",
    "    loss = F.cross_entropy(sim_matrix, positives)\n",
    "    return loss\n",
    "    \n",
    "\n",
    "# Pravljenje log fajla za cuvanje setup-a i pracenje rezultata\n",
    "save_dir = \"saved_models\"\n",
    "os.makedirs(save_dir, exist_ok = True)\n",
    "\n",
    "log_path = os.path.join(save_dir, f\"log_{MESSAGE}_{time.strftime('%Y%m%d-%H%M%S')}.txt\")\n",
    "with open(log_path, \"w\") as f:\n",
    "    f.write(\"SimCLR Training Log\\n\")\n",
    "    f.write(f\"MESSAGE: {MESSAGE}\\n\")\n",
    "    f.write(f\"BATCH_SIZE: {BATCH_SIZE}\\n\")\n",
    "    f.write(f\"EPOCHS: {EPOCHS}\\n\")\n",
    "    f.write(f\"TEMPERATURE: {TEMPERATURE}\\n\")\n",
    "    f.write(f\"LEARNING_RATE: {LR}\\n\")\n",
    "    f.write(f\"DEVICE: {DEVICE}\\n\")\n",
    "    f.write(f\"MODEL_SAVE_DIR: {save_dir}\\n\")\n",
    "    f.write(\"-\" * 40 + \"\\n\\n\")\n",
    "\n",
    "# Pripremamo i zapocinjemo trening modela\n",
    "# Cuvamo istreninarni model iz zadnje epohe\n",
    "model.train()\n",
    "    \n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    total_loss = 0\n",
    "    for (xi, xj) in tqdm(dataload):\n",
    "\n",
    "        xi, xj = xi.to(DEVICE), xj.to(DEVICE)\n",
    "        zi = model(xi)\n",
    "        zj = model(xj)\n",
    "\n",
    "        loss = calculate_loss(zi, zj, TEMPERATURE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"epoch: {epoch+1} loss: {total_loss/len(dataload):.4f}\")\n",
    "\n",
    "    if epoch == EPOCHS - 1:\n",
    "        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        model_path = os.path.join(save_dir, f\"simclr_{MESSAGE}_{timestamp}.pt\")\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    avg_loss = total_loss / len(dataload)\n",
    "    log_line = f\"Epoch {epoch+1}: loss = {avg_loss:.4f}\\n\"\n",
    "\n",
    "    with open(log_path, \"a\") as f:\n",
    "        f.write(log_line)\n",
    "\n",
    "\n",
    "\n",
    "##### LINEARNA EVALUACIJA #######\n",
    "# Ovo nije deo SimCLR algortima vec test rada na jednoj od mogucih njegovih primena.\n",
    "# Konkretno nauceni model cemo iskoristiti na problem klasifikacije.\n",
    "# Enkoder modela zamrzavamo tako da ne uci nista vise, a projekcionu glavu odbacujemo.\n",
    "# Pravimo jednu linearnu mrezu koja spaja samo izlaz iz enkodera sa klasama.\n",
    "# Potreban je uraditi i trening te mreze ali je to jednostavniji i vremenski dosta kraci proces jer su i dimenzije mnogo manje.\n",
    "\n",
    "# Ucitavanje slika i pravljenje setova za trening i test linearne evaluacije.\n",
    "cifar_train = torchvision.datasets.CIFAR10(root = \"./data\", train = True, download = True, transform = T.ToTensor())\n",
    "cifar_test = torchvision.datasets.CIFAR10(root = \"./data\", train = False, download = True, transform = T.ToTensor())\n",
    "\n",
    "train_load = DataLoader(cifar_train, batch_size = BATCH_SIZE, shuffle = True, num_workers = 4, pin_memory = True)\n",
    "test_load = DataLoader(cifar_test, batch_size = BATCH_SIZE, shuffle = False, num_workers = 2, pin_memory = True)\n",
    "\n",
    "# Zamrzavanje parametara modela naucenog SimCLR algoritmom.\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Mreza linearne klasifikacije.\n",
    "class LinearClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Definisanje modela klasifikatora, optimizatora za njega, kao i funkcije koja ce racunati loss \n",
    "classifier = LinearClassifier().to(DEVICE)\n",
    "optimizer_cl = torch.optim.Adam(classifier.parameters(), lr=1e-3)\n",
    "loss_cl = nn.CrossEntropyLoss()\n",
    "\n",
    "# Trening klasifikatora.\n",
    "# Dovoljno je da koristimo manji broj epoha jer vrednosti rano krenu da konvergiraju oko slicne vrednosti.\n",
    "classifier.train()\n",
    "for epoch in range(50):\n",
    "    \n",
    "    total_loss = 0\n",
    "    for x, y in train_load:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        h = model.encoder(x).squeeze()\n",
    "        pred = classifier(h)\n",
    "        loss = loss_cl(pred, y)\n",
    "\n",
    "        optimizer_cl.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_cl.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Linear epoch {epoch+1}, loss: {total_loss / len(train_load):.4f}\")\n",
    "    linear_log = f\"Linear Epoch {epoch+1}: loss = {total_loss / len(train_load):.4f}\\n\"\n",
    "    with open(log_path, \"a\") as f:\n",
    "        f.write(linear_log)\n",
    "\n",
    "\n",
    "# Konacna evaluacija koja radi sa test podacima.\n",
    "# Kao rezultat dobijamo konacan broj u procentima uspesnosti rada algoritma.\n",
    "def evaluate(model, classifier, dataloader):\n",
    "    model.eval()\n",
    "    classifier.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            h = model.encoder(x).squeeze()\n",
    "            out = classifier(h)\n",
    "            _, predicted = out.max(1)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            total += y.size(0)\n",
    "\n",
    "    print(f\"Test accuracy: {correct / total * 100:.2f}%\")\n",
    "    test_log = f\"Linear Evaluation Test Accuracy: {correct / total * 100:.2f}%\\n\"\n",
    "    with open(log_path, \"a\") as f:\n",
    "        f.write(test_log)\n",
    "\n",
    "\n",
    "evaluate(model, classifier, test_load)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
